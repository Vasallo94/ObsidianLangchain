{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3562, which is longer than the specified 1000\n",
      "Created a chunk of size 1095, which is longer than the specified 1000\n",
      "Created a chunk of size 1416, which is longer than the specified 1000\n",
      "Created a chunk of size 1156, which is longer than the specified 1000\n",
      "Created a chunk of size 1247, which is longer than the specified 1000\n",
      "Created a chunk of size 2388, which is longer than the specified 1000\n",
      "Created a chunk of size 1003, which is longer than the specified 1000\n",
      "Created a chunk of size 1250, which is longer than the specified 1000\n",
      "Created a chunk of size 1968, which is longer than the specified 1000\n",
      "Created a chunk of size 1398, which is longer than the specified 1000\n",
      "Created a chunk of size 2089, which is longer than the specified 1000\n",
      "/Users/enriquebook/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: La capacidad para aprender y mejorar continuamente, así como la importancia de la retroalimentación y la interacción con usuarios para ajustar y mejorar las habilidades y comportamiento.\n",
      "\n",
      "Fuentes:\n",
      "- Conversaciones con Llama3.1.md\n",
      "- Conversaciones con Llama3.1.md\n",
      "- Guion introspección.md\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ObsidianLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Cargar documentos de Obsidian\n",
    "loader = ObsidianLoader('/Users/enriquebook/Library/Mobile Documents/com~apple~CloudDocs/Obsidian/Secundo Selebro')\n",
    "documents = loader.load()\n",
    "\n",
    "# Dividir los documentos en chunks más pequeños\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Inicializar el modelo de embeddings de Hugging Face\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear la base de datos vectorial con Chroma\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Inicializar Ollama\n",
    "llm = Ollama(model=\"llama3.1\")\n",
    "\n",
    "# Crear una cadena de recuperación y respuesta\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Función para hacer preguntas\n",
    "def ask_question(question):\n",
    "    result = qa_chain({\"query\": question})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n",
    "\n",
    "# Ejemplo de uso\n",
    "question = \"¿Cuál es el tema principal de mis notas sobre inteligencia artificial?\"\n",
    "answer, sources = ask_question(question)\n",
    "print(f\"Respuesta: {answer}\\n\")\n",
    "print(\"Fuentes:\")\n",
    "for source in sources:\n",
    "    print(f\"- {source.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
